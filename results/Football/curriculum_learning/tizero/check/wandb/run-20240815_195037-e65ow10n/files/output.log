
사용 가능한 CPU Thread: 24
{'value_loss': 0.48991220504045485, 'policy_loss': -0.0013138270936906339, 'dist_entropy': 2.944152207374573, 'actor_grad_norm': tensor(0.2475), 'critic_grad_norm': tensor(1.6147), 'ratio': tensor(0.9831)}
Env Football Algo tizero Exp check updates 490/100000000000.0 steps in 10.38
total episode rewards is -7.969699859619141
{'value_loss': 0.8599551935866475, 'policy_loss': 0.006161661483347416, 'dist_entropy': 2.9437481641769407, 'actor_grad_norm': tensor(0.3614), 'critic_grad_norm': tensor(1.3611), 'ratio': tensor(0.8125)}
Env Football Algo tizero Exp check updates 896/100000000000.0 steps in 10.45
total episode rewards is 31.207501729329426
{'value_loss': 0.7986160890199244, 'policy_loss': -0.0018754961155354978, 'dist_entropy': 2.942508888244629, 'actor_grad_norm': tensor(0.3726), 'critic_grad_norm': tensor(1.1831), 'ratio': tensor(0.7217)}
Env Football Algo tizero Exp check updates 1256/100000000000.0 steps in 10.49
total episode rewards is 21.051897684733074
{'value_loss': 0.3292476256005466, 'policy_loss': -0.001589187476783991, 'dist_entropy': 2.9414066886901855, 'actor_grad_norm': tensor(0.3357), 'critic_grad_norm': tensor(0.6991), 'ratio': tensor(0.9531)}
Env Football Algo tizero Exp check updates 1732/100000000000.0 steps in 9.81
total episode rewards is -19.0562006632487
{'value_loss': 0.36406118909828367, 'policy_loss': -0.006332677379250526, 'dist_entropy': 2.9414731740951536, 'actor_grad_norm': tensor(0.3445), 'critic_grad_norm': tensor(0.4916), 'ratio': tensor(0.8763)}
Env Football Algo tizero Exp check updates 2170/100000000000.0 steps in 10.76
total episode rewards is 21.13490041097005
{'value_loss': 0.33898836794309317, 'policy_loss': -0.00037401380584924484, 'dist_entropy': 2.9403850746154787, 'actor_grad_norm': tensor(0.3402), 'critic_grad_norm': tensor(0.7839), 'ratio': tensor(0.8666)}
Env Football Algo tizero Exp check updates 2603/100000000000.0 steps in 10.35
total episode rewards is 1.1147003173828125
| eval_goal 0.1 | eval_goal_diff 0.1 | eval_win_rate 0.1 |
{'value_loss': 0.40556871564127506, 'policy_loss': -0.011913134180940688, 'dist_entropy': 2.9395453262329103, 'actor_grad_norm': tensor(0.3548), 'critic_grad_norm': tensor(0.6736), 'ratio': tensor(0.8993)}
Env Football Algo tizero Exp check updates 3052/100000000000.0 steps in 9.16
total episode rewards is 21.450599670410156
{'value_loss': 0.20877394624985754, 'policy_loss': 0.011421008803881704, 'dist_entropy': 2.9385610151290895, 'actor_grad_norm': tensor(0.2815), 'critic_grad_norm': tensor(0.4785), 'ratio': tensor(0.9082)}
Env Football Algo tizero Exp check updates 3506/100000000000.0 steps in 10.54
total episode rewards is 11.560099283854166
{'value_loss': 0.5971348434500396, 'policy_loss': -0.014739089491195045, 'dist_entropy': 2.938434386253357, 'actor_grad_norm': tensor(0.3721), 'critic_grad_norm': tensor(0.6565), 'ratio': tensor(0.7830)}
Env Football Algo tizero Exp check updates 3897/100000000000.0 steps in 9.72
total episode rewards is 31.21190134684245
{'value_loss': 0.20664802905172108, 'policy_loss': -0.005778864417225122, 'dist_entropy': 2.9384035778045656, 'actor_grad_norm': tensor(0.3421), 'critic_grad_norm': tensor(0.3714), 'ratio': tensor(0.9464)}
Env Football Algo tizero Exp check updates 4371/100000000000.0 steps in 9.47
total episode rewards is 11.40570068359375
{'value_loss': 0.4064068038854748, 'policy_loss': 0.0007990281353704631, 'dist_entropy': 2.937531361579895, 'actor_grad_norm': tensor(0.3386), 'critic_grad_norm': tensor(0.4217), 'ratio': tensor(0.9066)}
Env Football Algo tizero Exp check updates 4824/100000000000.0 steps in 9.06
total episode rewards is 0.6863002777099609
{'value_loss': 0.00495512287132442, 'policy_loss': -0.008346930124098435, 'dist_entropy': 2.9369522619247435, 'actor_grad_norm': tensor(0.5048), 'critic_grad_norm': tensor(0.1212), 'ratio': tensor(1.0005)}
Env Football Algo tizero Exp check updates 5324/100000000000.0 steps in 9.31
total episode rewards is 1.461000124613444
| eval_goal 0.0 | eval_goal_diff -0.2 | eval_win_rate 0.0 |
{'value_loss': 0.6125116057414561, 'policy_loss': -0.009156026830896734, 'dist_entropy': 2.937893419265747, 'actor_grad_norm': tensor(0.3594), 'critic_grad_norm': tensor(0.5068), 'ratio': tensor(0.7839)}
Env Football Algo tizero Exp check updates 5716/100000000000.0 steps in 10.75
total episode rewards is 10.788799285888672
{'value_loss': 0.22323249405715614, 'policy_loss': 0.004117032401263714, 'dist_entropy': 2.9369991779327393, 'actor_grad_norm': tensor(0.3375), 'critic_grad_norm': tensor(0.3870), 'ratio': tensor(0.9354)}
Env Football Algo tizero Exp check updates 6183/100000000000.0 steps in 9.34
total episode rewards is -8.845200220743815
{'value_loss': 0.6005149984080345, 'policy_loss': 0.002760330967139453, 'dist_entropy': 2.9362203073501587, 'actor_grad_norm': tensor(0.3599), 'critic_grad_norm': tensor(0.5103), 'ratio': tensor(0.7886)}
Env Football Algo tizero Exp check updates 6576/100000000000.0 steps in 9.34
total episode rewards is 11.405900319417318
{'value_loss': 0.43913838672451677, 'policy_loss': -0.0016085558891063556, 'dist_entropy': 2.9373202610015867, 'actor_grad_norm': tensor(0.3400), 'critic_grad_norm': tensor(0.7506), 'ratio': tensor(0.8263)}
Env Football Algo tizero Exp check updates 6989/100000000000.0 steps in 9.80
total episode rewards is 1.2055997848510742
{'value_loss': 0.663854252845049, 'policy_loss': -0.010683922688476741, 'dist_entropy': 2.9367358016967775, 'actor_grad_norm': tensor(0.3641), 'critic_grad_norm': tensor(0.7342), 'ratio': tensor(0.8865)}
Env Football Algo tizero Exp check updates 7430/100000000000.0 steps in 9.80
total episode rewards is 10.727099100748697
{'value_loss': 0.40936567729339, 'policy_loss': -0.013369526790920645, 'dist_entropy': 2.9366860437393187, 'actor_grad_norm': tensor(0.3281), 'critic_grad_norm': tensor(0.5876), 'ratio': tensor(0.8921)}
Env Football Algo tizero Exp check updates 7875/100000000000.0 steps in 8.86
total episode rewards is 21.667498270670574
| eval_goal 0.0 | eval_goal_diff 0.0 | eval_win_rate 0.0 |
{'value_loss': 0.004193764890078455, 'policy_loss': -0.01099930150900036, 'dist_entropy': 2.9351770305633544, 'actor_grad_norm': tensor(0.5361), 'critic_grad_norm': tensor(0.1039), 'ratio': tensor(0.9993)}
Env Football Algo tizero Exp check updates 8375/100000000000.0 steps in 9.37
total episode rewards is 1.1784000396728516
{'value_loss': 0.45318219179287555, 'policy_loss': -0.004186089280992746, 'dist_entropy': 2.9347272539138793, 'actor_grad_norm': tensor(0.3411), 'critic_grad_norm': tensor(0.5164), 'ratio': tensor(0.8966)}
Env Football Algo tizero Exp check updates 8824/100000000000.0 steps in 9.90
total episode rewards is 1.5097999572753906
{'value_loss': 0.23685522382613272, 'policy_loss': -0.011595187652856111, 'dist_entropy': 2.933697052001953, 'actor_grad_norm': tensor(0.3132), 'critic_grad_norm': tensor(0.3174), 'ratio': tensor(0.9342)}
Env Football Algo tizero Exp check updates 9291/100000000000.0 steps in 8.74
total episode rewards is 11.90280024210612
{'value_loss': 0.45935457220301035, 'policy_loss': -0.011481386853847652, 'dist_entropy': 2.933029932975769, 'actor_grad_norm': tensor(0.3310), 'critic_grad_norm': tensor(0.4166), 'ratio': tensor(0.9075)}
Env Football Algo tizero Exp check updates 9744/100000000000.0 steps in 9.56
total episode rewards is 1.622200647989909
{'value_loss': 0.36998661919496956, 'policy_loss': -0.002045193100348115, 'dist_entropy': 2.933791446685791, 'actor_grad_norm': tensor(0.3135), 'critic_grad_norm': tensor(0.2807), 'ratio': tensor(0.8043)}
Env Football Algo tizero Exp check updates 10146/100000000000.0 steps in 9.33
total episode rewards is 20.964701334635418
{'value_loss': 0.9015511257760227, 'policy_loss': -0.014121326418826357, 'dist_entropy': 2.9308497858047486, 'actor_grad_norm': tensor(0.4052), 'critic_grad_norm': tensor(0.9320), 'ratio': tensor(0.8611)}
Env Football Algo tizero Exp check updates 10577/100000000000.0 steps in 9.51
total episode rewards is 1.4557005564371746
| eval_goal 0.2 | eval_goal_diff 0.2 | eval_win_rate 0.2 |
{'value_loss': 1.047264021858573, 'policy_loss': -0.009968134122900665, 'dist_entropy': 2.930953869819641, 'actor_grad_norm': tensor(0.4092), 'critic_grad_norm': tensor(0.9109), 'ratio': tensor(0.6983)}
Env Football Algo tizero Exp check updates 10925/100000000000.0 steps in 10.97
total episode rewards is 30.756601969401043
{'value_loss': 0.5867236846871674, 'policy_loss': -0.0005884065758436918, 'dist_entropy': 2.929270725250244, 'actor_grad_norm': tensor(0.3939), 'critic_grad_norm': tensor(0.6141), 'ratio': tensor(0.7686)}
Env Football Algo tizero Exp check updates 11309/100000000000.0 steps in 9.89
total episode rewards is 30.94390106201172
{'value_loss': 0.7644279158487916, 'policy_loss': -0.019752969779074193, 'dist_entropy': 2.9297443723678587, 'actor_grad_norm': tensor(0.4331), 'critic_grad_norm': tensor(0.8097), 'ratio': tensor(0.6591)}
Env Football Algo tizero Exp check updates 11638/100000000000.0 steps in 10.50
total episode rewards is 40.93719991048177
{'value_loss': 0.39187391605228183, 'policy_loss': -0.007180393543094397, 'dist_entropy': 2.923986620903015, 'actor_grad_norm': tensor(0.4144), 'critic_grad_norm': tensor(0.6251), 'ratio': tensor(0.8576)}
Env Football Algo tizero Exp check updates 12066/100000000000.0 steps in 10.54
total episode rewards is 21.228200276692707
{'value_loss': 0.3710065229050815, 'policy_loss': 0.006796831305837259, 'dist_entropy': 2.922783455848694, 'actor_grad_norm': tensor(0.4480), 'critic_grad_norm': tensor(0.7449), 'ratio': tensor(0.8419)}
Env Football Algo tizero Exp check updates 12487/100000000000.0 steps in 10.81
total episode rewards is 0.5534997781117758
{'value_loss': 0.6973637612350285, 'policy_loss': -0.0017007338907569648, 'dist_entropy': 2.9250136041641235, 'actor_grad_norm': tensor(0.4187), 'critic_grad_norm': tensor(0.7272), 'ratio': tensor(0.6563)}
Env Football Algo tizero Exp check updates 12815/100000000000.0 steps in 10.51
total episode rewards is 20.65109634399414
{'value_loss': 0.7647486674785614, 'policy_loss': -0.012647934695705772, 'dist_entropy': 2.9233358335494994, 'actor_grad_norm': tensor(0.4543), 'critic_grad_norm': tensor(0.8311), 'ratio': tensor(0.6866)}
Env Football Algo tizero Exp check updates 13158/100000000000.0 steps in 11.32
total episode rewards is 20.599398295084637
| eval_goal 0.0 | eval_goal_diff 0.0 | eval_win_rate 0.0 |
{'value_loss': 0.6771947685629129, 'policy_loss': -0.004852468483150006, 'dist_entropy': 2.922939648628235, 'actor_grad_norm': tensor(0.4409), 'critic_grad_norm': tensor(0.8733), 'ratio': tensor(0.6281)}
Env Football Algo tizero Exp check updates 13471/100000000000.0 steps in 11.55
total episode rewards is 40.634300231933594
{'value_loss': 0.562508737668395, 'policy_loss': 0.013721870956942439, 'dist_entropy': 2.9186799907684327, 'actor_grad_norm': tensor(0.3982), 'critic_grad_norm': tensor(1.0338), 'ratio': tensor(0.7501)}
Env Football Algo tizero Exp check updates 13845/100000000000.0 steps in 10.11
total episode rewards is 10.544699986775717
{'value_loss': 0.8628664989769459, 'policy_loss': -0.003760714759118855, 'dist_entropy': 2.922864317893982, 'actor_grad_norm': tensor(0.5286), 'critic_grad_norm': tensor(1.1221), 'ratio': tensor(0.6359)}
Env Football Algo tizero Exp check updates 14163/100000000000.0 steps in 9.85
total episode rewards is 30.944600423177082
{'value_loss': 0.18542822973802686, 'policy_loss': -0.010740327443927526, 'dist_entropy': 2.9083676862716676, 'actor_grad_norm': tensor(0.3485), 'critic_grad_norm': tensor(0.6047), 'ratio': tensor(0.9085)}
Env Football Algo tizero Exp check updates 14616/100000000000.0 steps in 10.68
total episode rewards is 10.938098907470703
{'value_loss': 0.1969773311726749, 'policy_loss': -0.0006595050683245063, 'dist_entropy': 2.905191102027893, 'actor_grad_norm': tensor(0.3415), 'critic_grad_norm': tensor(0.4269), 'ratio': tensor(0.9528)}
Env Football Algo tizero Exp check updates 15092/100000000000.0 steps in 9.48
total episode rewards is 11.21530024210612
{'value_loss': 0.36369491409510374, 'policy_loss': -0.0013241102022584527, 'dist_entropy': 2.902583041191101, 'actor_grad_norm': tensor(0.3901), 'critic_grad_norm': tensor(0.4776), 'ratio': tensor(0.8975)}
Env Football Algo tizero Exp check updates 15541/100000000000.0 steps in 10.29
total episode rewards is 0.9212004343668619
{'value_loss': 0.5293691617436707, 'policy_loss': -0.010438894247636199, 'dist_entropy': 2.902492489814758, 'actor_grad_norm': tensor(0.3726), 'critic_grad_norm': tensor(0.7734), 'ratio': tensor(0.8802)}
Env Football Algo tizero Exp check updates 15981/100000000000.0 steps in 9.56
total episode rewards is 10.894700368245443
| eval_goal 0.9 | eval_goal_diff 0.9 | eval_win_rate 0.9 |
{'value_loss': 0.3499651216715574, 'policy_loss': -0.022366221607662738, 'dist_entropy': 2.904906806945801, 'actor_grad_norm': tensor(0.3746), 'critic_grad_norm': tensor(0.4984), 'ratio': tensor(0.8130)}
Env Football Algo tizero Exp check updates 16387/100000000000.0 steps in 10.54
total episode rewards is 21.352699279785156
{'value_loss': 0.1537293984275311, 'policy_loss': -0.012517062732949853, 'dist_entropy': 2.892700710296631, 'actor_grad_norm': tensor(0.3552), 'critic_grad_norm': tensor(0.4823), 'ratio': tensor(0.9046)}
Env Football Algo tizero Exp check updates 16838/100000000000.0 steps in 7.97
total episode rewards is 10.444400151570639
{'value_loss': 0.00671131367329508, 'policy_loss': -0.010071667963638902, 'dist_entropy': 2.8888799858093264, 'actor_grad_norm': tensor(0.7497), 'critic_grad_norm': tensor(0.1637), 'ratio': tensor(1.0009)}
Env Football Algo tizero Exp check updates 17338/100000000000.0 steps in 10.03
total episode rewards is 0.9633000691731771
{'value_loss': 0.5911768050305546, 'policy_loss': -0.02351133661228232, 'dist_entropy': 2.8897713375091554, 'actor_grad_norm': tensor(0.4012), 'critic_grad_norm': tensor(0.7031), 'ratio': tensor(0.8134)}
Env Football Algo tizero Exp check updates 17745/100000000000.0 steps in 11.59
total episode rewards is 30.97039794921875
{'value_loss': 0.0036354317865334453, 'policy_loss': -0.010566702177748084, 'dist_entropy': 2.872485032081604, 'actor_grad_norm': tensor(0.6534), 'critic_grad_norm': tensor(0.0879), 'ratio': tensor(0.9991)}
Env Football Algo tizero Exp check updates 18245/100000000000.0 steps in 9.13
total episode rewards is 0.8125000794728597
{'value_loss': 0.8559814952686429, 'policy_loss': -0.025030821828113402, 'dist_entropy': 2.903076343536377, 'actor_grad_norm': tensor(0.5277), 'critic_grad_norm': tensor(0.4424), 'ratio': tensor(0.5879)}
Env Football Algo tizero Exp check updates 18539/100000000000.0 steps in 11.38
total episode rewards is 50.62199910481771
| eval_goal 0.0 | eval_goal_diff 0.0 | eval_win_rate 0.0 |
{'value_loss': 0.5071024622023106, 'policy_loss': -0.01431487649679184, 'dist_entropy': 2.8894847202301026, 'actor_grad_norm': tensor(0.4015), 'critic_grad_norm': tensor(0.5135), 'ratio': tensor(0.7374)}
Env Football Algo tizero Exp check updates 18908/100000000000.0 steps in 10.28
total episode rewards is 30.805796305338543
{'value_loss': 0.5234455960802734, 'policy_loss': -0.014255496771074832, 'dist_entropy': 2.885493745803833, 'actor_grad_norm': tensor(0.4433), 'critic_grad_norm': tensor(0.4575), 'ratio': tensor(0.7807)}
Env Football Algo tizero Exp check updates 19296/100000000000.0 steps in 8.94
total episode rewards is 10.944798787434896
Traceback (most recent call last):
  File "/home/uosai/Desktop/marl/onpolicy/train_grf.py", line 238, in <module>
    main(args = sys.argv[1:])
  File "/home/uosai/Desktop/marl/onpolicy/train_grf.py", line 229, in main
    runner.run()
  File "/home/uosai/Desktop/marl/onpolicy/runner/shared/football_runner.py", line 65, in run
    values, actions, action_log_probs, rnn_states, rnn_states_critic, actions_env, enemy_rnn_states = self.collect(step)
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/uosai/Desktop/marl/onpolicy/runner/shared/football_runner.py", line 188, in collect
    values, actions, action_log_probs, rnn_states, rnn_states_critic= self.trainer.policy.get_actions(
  File "/home/uosai/Desktop/marl/onpolicy/algorithms/tizero/algorithm/TiZeroPolicy.py", line 73, in get_actions
    values, rnn_states_critic = self.critic(cent_obs, rnn_states_critic, masks)
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/uosai/Desktop/marl/onpolicy/algorithms/tizero/algorithm/r_actor_critic.py", line 192, in forward
    x, rnn_states = self.obs_encoder(cent_obs, rnn_states, masks)
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/uosai/Desktop/marl/onpolicy/algorithms/utils/input_encoder.py", line 398, in forward
    actor_features = self.input_encoder(obs)
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/uosai/Desktop/marl/onpolicy/algorithms/utils/input_encoder.py", line 349, in forward
    match_state_vec = x[:, self.ball_info_num + self.ball_owner_num + self.left_input_num + self.right_input_num:]
KeyboardInterrupt