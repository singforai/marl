
사용 가능한 CPU Thread: 24
{'value_loss': 0.5800327065587044, 'policy_loss': 0.015577714618993923, 'dist_entropy': 2.944199323654175, 'actor_grad_norm': tensor(0.2060), 'critic_grad_norm': tensor(1.2457), 'ratio': tensor(0.7936)}
Env Football Algo tizero Exp check updates 1190/100000000000.0 steps in 23.31
total episode rewards is -10.529702504475912
{'value_loss': 0.27901176299899816, 'policy_loss': 0.024806514617521317, 'dist_entropy': 2.9437027645111082, 'actor_grad_norm': tensor(0.2051), 'critic_grad_norm': tensor(0.7216), 'ratio': tensor(0.8748)}
Env Football Algo tizero Exp check updates 2502/100000000000.0 steps in 21.42
total episode rewards is -0.5156022707621256
{'value_loss': 0.4140523920580745, 'policy_loss': 0.053164585498161616, 'dist_entropy': 2.942507290840149, 'actor_grad_norm': tensor(0.2146), 'critic_grad_norm': tensor(0.6359), 'ratio': tensor(0.7633)}
Env Football Algo tizero Exp check updates 3647/100000000000.0 steps in 24.32
total episode rewards is 9.546595891316732
{'value_loss': 0.6267750726267696, 'policy_loss': 0.009310470018535853, 'dist_entropy': 2.9407387733459474, 'actor_grad_norm': tensor(0.2170), 'critic_grad_norm': tensor(0.5618), 'ratio': tensor(0.5949)}
Env Football Algo tizero Exp check updates 4538/100000000000.0 steps in 25.90
total episode rewards is 49.69829813639323
{'value_loss': 0.586916733905673, 'policy_loss': -0.009670758545398712, 'dist_entropy': 2.939934115409851, 'actor_grad_norm': tensor(0.2422), 'critic_grad_norm': tensor(0.5599), 'ratio': tensor(0.6420)}
Env Football Algo tizero Exp check updates 5500/100000000000.0 steps in 25.38
total episode rewards is 49.52618916829427
| eval_goal 0.1 | eval_goal_diff 0.1 | eval_WDL 0.1 |
{'value_loss': 0.8250643754005432, 'policy_loss': -0.010638162281829865, 'dist_entropy': 2.940258331298828, 'actor_grad_norm': tensor(0.2631), 'critic_grad_norm': tensor(0.5255), 'ratio': tensor(0.6314)}
Env Football Algo tizero Exp check updates 6446/100000000000.0 steps in 24.99
total episode rewards is 59.61259460449219
{'value_loss': 0.4911724628508091, 'policy_loss': 0.0030798972041702656, 'dist_entropy': 2.939405794143677, 'actor_grad_norm': tensor(0.2351), 'critic_grad_norm': tensor(0.5681), 'ratio': tensor(0.6355)}
Env Football Algo tizero Exp check updates 7398/100000000000.0 steps in 24.80
total episode rewards is -10.326800664265951
{'value_loss': 0.6188717194646597, 'policy_loss': 0.006551205378491432, 'dist_entropy': 2.940901436805725, 'actor_grad_norm': tensor(0.2667), 'critic_grad_norm': tensor(0.6444), 'ratio': tensor(0.4857)}
Env Football Algo tizero Exp check updates 8126/100000000000.0 steps in 24.00
total episode rewards is 29.676600138346355
{'value_loss': 0.42098778769373896, 'policy_loss': 0.029731729849008844, 'dist_entropy': 2.9374746322631835, 'actor_grad_norm': tensor(0.2412), 'critic_grad_norm': tensor(0.4686), 'ratio': tensor(0.6087)}
Env Football Algo tizero Exp check updates 9040/100000000000.0 steps in 23.76
total episode rewards is -10.40530522664388
{'value_loss': 0.34194774720817805, 'policy_loss': -0.004204735355451703, 'dist_entropy': 2.9366874217987062, 'actor_grad_norm': tensor(0.2306), 'critic_grad_norm': tensor(0.4903), 'ratio': tensor(0.7117)}
Env Football Algo tizero Exp check updates 10107/100000000000.0 steps in 25.88
total episode rewards is 19.516993204752605
{'value_loss': 0.37439001297578217, 'policy_loss': 0.010031249647618097, 'dist_entropy': 2.935508484840393, 'actor_grad_norm': tensor(0.2938), 'critic_grad_norm': tensor(0.6075), 'ratio': tensor(0.8355)}
Env Football Algo tizero Exp check updates 11361/100000000000.0 steps in 23.45
total episode rewards is -0.40810203552246094
| eval_goal 0.1 | eval_goal_diff 0.0 | eval_WDL 0.1 |
{'value_loss': 0.6289120101928711, 'policy_loss': 0.023557930439710616, 'dist_entropy': 2.936199073791504, 'actor_grad_norm': tensor(0.2986), 'critic_grad_norm': tensor(0.6591), 'ratio': tensor(0.6366)}
Env Football Algo tizero Exp check updates 12315/100000000000.0 steps in 25.38
total episode rewards is -30.38890329996745
{'value_loss': 0.36210526019334793, 'policy_loss': 0.019127061045728623, 'dist_entropy': 2.935300350189209, 'actor_grad_norm': tensor(0.2628), 'critic_grad_norm': tensor(0.6697), 'ratio': tensor(0.7212)}
Env Football Algo tizero Exp check updates 13396/100000000000.0 steps in 24.18
total episode rewards is 39.68819681803385
{'value_loss': 0.5162379529327155, 'policy_loss': 0.005318435210974712, 'dist_entropy': 2.932747793197632, 'actor_grad_norm': tensor(0.2506), 'critic_grad_norm': tensor(0.5124), 'ratio': tensor(0.6194)}
Env Football Algo tizero Exp check updates 14323/100000000000.0 steps in 28.80
total episode rewards is -0.4365033308664958
{'value_loss': 0.18473498677834868, 'policy_loss': -0.0029407190287020057, 'dist_entropy': 2.9249043226242066, 'actor_grad_norm': tensor(0.2593), 'critic_grad_norm': tensor(0.3553), 'ratio': tensor(0.9423)}
Env Football Algo tizero Exp check updates 15737/100000000000.0 steps in 25.06
total episode rewards is -0.3955998420715332
{'value_loss': 0.44635133612900973, 'policy_loss': -0.0030256687640212478, 'dist_entropy': 2.930520176887512, 'actor_grad_norm': tensor(0.2992), 'critic_grad_norm': tensor(0.4290), 'ratio': tensor(0.8393)}
Env Football Algo tizero Exp check updates 16995/100000000000.0 steps in 25.52
total episode rewards is 29.503799438476562
| eval_goal 0.1 | eval_goal_diff 0.1 | eval_WDL 0.1 |
{'value_loss': 0.6811535289138555, 'policy_loss': -0.005052872849628329, 'dist_entropy': 2.9364738941192625, 'actor_grad_norm': tensor(0.3129), 'critic_grad_norm': tensor(0.5771), 'ratio': tensor(0.4908)}
Env Football Algo tizero Exp check updates 17729/100000000000.0 steps in 25.96
total episode rewards is 59.71619669596354
{'value_loss': 0.6769260019809008, 'policy_loss': -0.0056254379951860755, 'dist_entropy': 2.9359475564956665, 'actor_grad_norm': tensor(0.3100), 'critic_grad_norm': tensor(0.4921), 'ratio': tensor(0.4932)}
Env Football Algo tizero Exp check updates 18467/100000000000.0 steps in 26.00
total episode rewards is 39.69029744466146
{'value_loss': 0.6097709833085537, 'policy_loss': -0.017555680975783618, 'dist_entropy': 2.934807152748108, 'actor_grad_norm': tensor(0.3012), 'critic_grad_norm': tensor(0.5651), 'ratio': tensor(0.5088)}
Env Football Algo tizero Exp check updates 19227/100000000000.0 steps in 28.13
total episode rewards is 49.632100423177086
{'value_loss': 0.2542685959115624, 'policy_loss': 0.010271838591434062, 'dist_entropy': 2.9252880382537843, 'actor_grad_norm': tensor(0.2805), 'critic_grad_norm': tensor(0.4494), 'ratio': tensor(0.8351)}
Env Football Algo tizero Exp check updates 20479/100000000000.0 steps in 23.80
total episode rewards is 9.59009869893392
{'value_loss': 0.6493319183588028, 'policy_loss': 0.022055279803462325, 'dist_entropy': 2.9333333110809328, 'actor_grad_norm': tensor(0.3134), 'critic_grad_norm': tensor(0.6639), 'ratio': tensor(0.4995)}
Env Football Algo tizero Exp check updates 21226/100000000000.0 steps in 26.26
total episode rewards is 19.630503336588543
{'value_loss': 0.40258346512913706, 'policy_loss': 0.02312605967395939, 'dist_entropy': 2.929988350868225, 'actor_grad_norm': tensor(0.2845), 'critic_grad_norm': tensor(0.5780), 'ratio': tensor(0.5704)}
Env Football Algo tizero Exp check updates 22080/100000000000.0 steps in 28.26
total episode rewards is 29.708302815755207
| eval_goal 0.0 | eval_goal_diff 0.0 | eval_WDL 0.0 |
{'value_loss': 0.5700004230439663, 'policy_loss': 0.027769106486812235, 'dist_entropy': 2.926923294067383, 'actor_grad_norm': tensor(0.3139), 'critic_grad_norm': tensor(0.7954), 'ratio': tensor(0.5714)}
Env Football Algo tizero Exp check updates 22935/100000000000.0 steps in 25.55
total episode rewards is 29.790494283040363
{'value_loss': 0.49972375683486464, 'policy_loss': 0.01624749928363599, 'dist_entropy': 2.9260451126098634, 'actor_grad_norm': tensor(0.3083), 'critic_grad_norm': tensor(0.5788), 'ratio': tensor(0.6112)}
Env Football Algo tizero Exp check updates 23851/100000000000.0 steps in 25.15
total episode rewards is -20.260904947916668
{'value_loss': 0.47931899212300777, 'policy_loss': 0.04901405306532979, 'dist_entropy': 2.9247186613082885, 'actor_grad_norm': tensor(0.3203), 'critic_grad_norm': tensor(0.7148), 'ratio': tensor(0.6424)}
Env Football Algo tizero Exp check updates 24813/100000000000.0 steps in 26.82
total episode rewards is -20.34420394897461
{'value_loss': 0.5495248816162348, 'policy_loss': 0.020188708934001626, 'dist_entropy': 2.9248226833343507, 'actor_grad_norm': tensor(0.3135), 'critic_grad_norm': tensor(0.6387), 'ratio': tensor(0.5696)}
Env Football Algo tizero Exp check updates 25668/100000000000.0 steps in 25.90
total episode rewards is 29.598795572916668
{'value_loss': 0.4697794607281685, 'policy_loss': 0.034187790093710646, 'dist_entropy': 2.9217268514633177, 'actor_grad_norm': tensor(0.3483), 'critic_grad_norm': tensor(0.7287), 'ratio': tensor(0.6457)}
Env Football Algo tizero Exp check updates 26635/100000000000.0 steps in 26.58
total episode rewards is 19.702796936035156
{'value_loss': 0.24549817144870759, 'policy_loss': 0.018752327170222998, 'dist_entropy': 2.916502499580383, 'actor_grad_norm': tensor(0.2908), 'critic_grad_norm': tensor(0.6012), 'ratio': tensor(0.7831)}
Env Football Algo tizero Exp check updates 27808/100000000000.0 steps in 27.70
total episode rewards is 29.589998881022137
| eval_goal 0.2 | eval_goal_diff 0.2 | eval_WDL 0.2 |
{'value_loss': 0.4099096054583788, 'policy_loss': 0.0035950123448856173, 'dist_entropy': 2.9162555932998657, 'actor_grad_norm': tensor(0.2963), 'critic_grad_norm': tensor(0.6157), 'ratio': tensor(0.6626)}
Env Football Algo tizero Exp check updates 28801/100000000000.0 steps in 24.46
total episode rewards is 29.663798014322918
{'value_loss': 0.317997682467103, 'policy_loss': 0.012465191734954716, 'dist_entropy': 2.910357689857483, 'actor_grad_norm': tensor(0.3717), 'critic_grad_norm': tensor(0.5272), 'ratio': tensor(0.8291)}
Env Football Algo tizero Exp check updates 30044/100000000000.0 steps in 25.29
total episode rewards is -20.42409896850586
Traceback (most recent call last):
  File "/home/uosai/Desktop/marl/onpolicy/train_grf.py", line 238, in <module>
    main(args = sys.argv[1:])
  File "/home/uosai/Desktop/marl/onpolicy/train_grf.py", line 229, in main
    runner.run()
  File "/home/uosai/Desktop/marl/onpolicy/runner/shared/football_runner.py", line 66, in run
    obs, rewards, dones, infos = self.envs.step(actions_env)
  File "/home/uosai/Desktop/marl/onpolicy/envs/env_wrappers.py", line 107, in step
    return self.step_wait()
  File "/home/uosai/Desktop/marl/onpolicy/envs/env_wrappers.py", line 263, in step_wait
    results = [remote.recv() for remote in self.remotes]
  File "/home/uosai/Desktop/marl/onpolicy/envs/env_wrappers.py", line 263, in <listcomp>
    results = [remote.recv() for remote in self.remotes]
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
KeyboardInterrupt