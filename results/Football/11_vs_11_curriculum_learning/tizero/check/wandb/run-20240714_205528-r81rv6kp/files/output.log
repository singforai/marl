/home/uosai/Desktop/marl/onpolicy/envs/package/gym/core.py:317: DeprecationWarning: [33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.
  deprecation(
ì‚¬ìš© ê°€ëŠ¥í•œ CPU Thread: 24
ì „ì²´ ê±¸ë¦° ì‹œê°„: 0ì‹œê°„ 0ë¶„ 27.42ì´ˆ
Traceback (most recent call last):
  File "/home/uosai/Desktop/marl/onpolicy/train_grf.py", line 215, in <module>
    main(args = sys.argv[1:])
  File "/home/uosai/Desktop/marl/onpolicy/train_grf.py", line 206, in main
    runner.run()
  File "/home/uosai/Desktop/marl/onpolicy/runner/shared/football_runner.py", line 76, in run
    train_infos = self.train()
  File "/home/uosai/Desktop/marl/onpolicy/runner/shared/base_runner.py", line 178, in train
    train_infos = self.trainer.train(self.buffer)
  File "/home/uosai/Desktop/marl/onpolicy/algorithms/tizero/tizero.py", line 248, in train
    = self.ppo_update(sample, update_actor)
  File "/home/uosai/Desktop/marl/onpolicy/algorithms/tizero/tizero.py", line 135, in ppo_update
    values, action_log_probs, dist_entropy = self.policy.evaluate_actions(
  File "/home/uosai/Desktop/marl/onpolicy/algorithms/tizero/algorithm/TiZeroPolicy.py", line 109, in evaluate_actions
    action_log_probs, dist_entropy = self.actor.evaluate_actions(obs,
  File "/home/uosai/Desktop/marl/onpolicy/algorithms/tizero/algorithm/r_actor_critic.py", line 125, in evaluate_actions
    actor_features = self.base(obs)
  File "/home/uosai/Desktop/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'R_Actor' object has no attribute 'base'