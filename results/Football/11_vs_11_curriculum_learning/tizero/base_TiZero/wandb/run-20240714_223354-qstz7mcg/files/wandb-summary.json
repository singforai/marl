{"value_loss": 0.31715618041654425, "_timestamp": 1721021805.9876702, "policy_loss": 0.0030820745820528826, "dist_entropy": 1.3951029698053996, "actor_grad_norm": 0.3109690546989441, "critic_grad_norm": 0.6253378391265869, "ratio": 1.222890019416809, "average_episode_rewards": -14.999999664723873, "train_possession_rate": 0.7539666666666667, "_runtime": 57771.80052113533, "_step": 10620000, "train_goal_diff": -2.9, "train_goal": 0.0, "train_WDL": -1.0, "eval_goal": 0.0, "eval_WDL": -1.0, "eval_goal_diff": -3.1}