{"value_loss": 0.5100473214189212, "_timestamp": 1721021803.8866773, "policy_loss": -0.03992043265781831, "dist_entropy": 1.9003610324859619, "actor_grad_norm": 0.28767895698547363, "critic_grad_norm": 1.0244790315628052, "ratio": 1.5107309818267822, "average_episode_rewards": -35.00000014901161, "train_possession_rate": 0.718, "_runtime": 57769.74016022682, "_step": 10680000, "train_goal_diff": -3.5, "train_goal": 0.0, "train_WDL": -1.0, "eval_goal": 0.0, "eval_WDL": -1.0, "eval_goal_diff": -3.5}