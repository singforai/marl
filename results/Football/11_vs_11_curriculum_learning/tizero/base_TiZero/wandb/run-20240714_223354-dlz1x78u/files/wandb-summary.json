{"value_loss": 0.5155093570053577, "_timestamp": 1721021677.3652976, "policy_loss": -0.033179175652718794, "dist_entropy": 1.7521470594406128, "actor_grad_norm": 0.30504748225212097, "critic_grad_norm": 0.8264139890670776, "ratio": 1.4502415657043457, "average_episode_rewards": -28.999999165534973, "train_possession_rate": 0.7520666666666668, "_runtime": 57643.189668655396, "_step": 10320000, "train_goal_diff": -2.9, "train_goal": 0.0, "train_WDL": -1.0, "eval_goal": 0.3, "eval_WDL": -0.9, "eval_goal_diff": -2.5}