{"value_loss": 0.3958626261601845, "_timestamp": 1721021768.2076523, "policy_loss": -0.0021794224370387384, "dist_entropy": 1.678886829217275, "actor_grad_norm": 0.36401858925819397, "critic_grad_norm": 0.8636969327926636, "ratio": 1.4220274686813354, "average_episode_rewards": -21.000000648200512, "train_possession_rate": 0.7215333333333332, "_runtime": 57734.02750134468, "_step": 10410000, "train_goal_diff": -2.5, "train_goal": 0.2, "train_WDL": -0.9, "eval_goal": 0.1, "eval_WDL": -1.0, "eval_goal_diff": -2.4}