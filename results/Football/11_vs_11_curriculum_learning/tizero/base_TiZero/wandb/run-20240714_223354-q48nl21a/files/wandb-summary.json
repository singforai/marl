{"value_loss": 0.406019338046511, "_timestamp": 1721021646.846507, "policy_loss": -0.029708164557038497, "dist_entropy": 1.869150887330373, "actor_grad_norm": 0.28445789217948914, "critic_grad_norm": 0.9134413003921509, "ratio": 1.4614241123199463, "average_episode_rewards": -22.999999579042196, "train_possession_rate": 0.6930666666666666, "_runtime": 57612.703099012375, "_step": 10320000, "train_goal_diff": -2.8, "train_goal": 0.1, "train_WDL": -0.9, "eval_goal": 0.0, "eval_WDL": -1.0, "eval_goal_diff": -2.1}