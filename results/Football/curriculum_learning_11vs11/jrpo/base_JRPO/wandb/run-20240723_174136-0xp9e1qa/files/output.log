
사용 가능한 CPU Thread: 128
Env Football Algo jrpo Exp base_JRPO updates 0/1666666 episodes total num timesteps 60000/100000000000.0
average episode rewards is -35.00000014901161
Env Football Algo jrpo Exp base_JRPO updates 1/1666666 episodes total num timesteps 120000/100000000000.0
average episode rewards is -21.499999333173037
Env Football Algo jrpo Exp base_JRPO updates 2/1666666 episodes total num timesteps 180000/100000000000.0
average episode rewards is -31.499999575316906
Env Football Algo jrpo Exp base_JRPO updates 3/1666666 episodes total num timesteps 240000/100000000000.0
average episode rewards is -31.499999575316906
Env Football Algo jrpo Exp base_JRPO updates 4/1666666 episodes total num timesteps 300000/100000000000.0
average episode rewards is -25.999998673796654
Traceback (most recent call last):
  File "/root/marl/onpolicy/train_grf.py", line 221, in <module>
    main(args = sys.argv[1:])
  File "/root/marl/onpolicy/train_grf.py", line 212, in main
    runner.run()
  File "/root/marl/onpolicy/runner/shared/football_runner.py", line 120, in run
    self.eval(total_num_steps)
  File "/root/miniconda3/envs/marl/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/root/marl/onpolicy/runner/shared/football_runner.py", line 240, in eval
    eval_actions, eval_rnn_states = self.trainer.policy.act(
  File "/root/marl/onpolicy/algorithms/r_mappo/algorithm/rMAPPOPolicy.py", line 129, in act
    actions, _, rnn_states_actor = self.actor(obs, rnn_states_actor, masks, available_actions, deterministic)
  File "/root/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/marl/onpolicy/algorithms/r_mappo/algorithm/r_actor_critic.py", line 64, in forward
    actor_features = self.base(obs)
  File "/root/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/marl/onpolicy/algorithms/utils/mlp.py", line 54, in forward
    x = self.feature_norm(x)
  File "/root/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/modules/normalization.py", line 201, in forward
    return F.layer_norm(
  File "/root/miniconda3/envs/marl/lib/python3.9/site-packages/torch/nn/functional.py", line 2573, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Given normalized_shape=[115], expected input with shape [*, 115], but got input of size[100, 330]